{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re, csv, sys, nltk, PyPDF2, glob, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the CSV file and convert it into two dictionaries: one for unigrams and one for bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv_file = 'ESGword.csv'\n",
    "unigram_dict = defaultdict(list)\n",
    "bigram_dict = defaultdict(list)\n",
    "\n",
    "# getting ESG words from csv file and storing in dictionaries\n",
    "with open(csv_file, 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "\n",
    "    for row in reader:\n",
    "        for column_header, column_value in row.items():\n",
    "            # Determine whether it's a bigram or unigram based on the column header\n",
    "            is_bigram = 'bigrams' in column_header.lower()\n",
    "\n",
    "            # Extract the key (first word of the column header) and convert to lowercase\n",
    "            key = column_header.split()[0].lower()\n",
    "\n",
    "            # Add the non-empty, lowercase values to the appropriate dictionary\n",
    "            values = [value.strip().lower() for value in column_value.split(',') if value.strip()]\n",
    "            if is_bigram:\n",
    "                bigram_dict[key].extend(values)\n",
    "            else:\n",
    "                unigram_dict[key].extend(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract pdf text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extractText(filepath):\n",
    "    pdfFileObj = open(filepath, 'rb')\n",
    "     \n",
    "    # creating a pdf reader object\n",
    "    pdfReader = PyPDF2.PdfReader(pdfFileObj)\n",
    "     \n",
    "    # extracting text from page\n",
    "    combined_text = ''\n",
    "    # Print out all the text.\n",
    "    for info in pdfReader.pages:\n",
    "        combined_text = combined_text + ' '+ info.extract_text()\n",
    "        \n",
    "    text = re.sub(\"\\n\", \" \", combined_text)\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    " \n",
    "    # closing the pdf file object\n",
    "    pdfFileObj.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Print Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_fdist(items, num_common=20):\n",
    "    fdist = nltk.FreqDist(items)\n",
    "    most_common = fdist.most_common(num_common)\n",
    "    print(*most_common, sep=\"\\n\")\n",
    "    print(\"\")\n",
    "    return fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Tokenize all the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def alltokens(text):\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "punct_list = [\".\", \",\", \"?\", \"“\", \"”\", \"’\", \";\", \"!\", '—', '‘',\"''\",'``','%','•', \"*\", '$', ':', '&', '(', ')', '|','<','-','–','/']\n",
    "\n",
    "stoplist = stopwords.words('english') \n",
    "stoplist.extend(punct_list)\n",
    "stoplist.extend([\"much\", \"like\", \"one\", \"many\", \"though\", \"without\", \"upon\",\"also\",\"'s\",\"may\",\"across\",\"part\", \"percent\",\"could\",\"would\",'often','usually'])\n",
    "stoplist.extend(['data','nr','fiscal','table','index','u.s.','reporting','company','percentage','statements','includes','use'])\n",
    "stoplist.extend(['total','business','continued','content','including','overview','year','number','category','pp','new','gid','/gid','pages','page','nr','p.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def autopct_format(values):\n",
    "    def my_format(pct):\n",
    "        total = sum(values)\n",
    "        val = int(round(pct*total/100.0))\n",
    "        return '{:.1f}%\\n({v:d})'.format(pct, v=val)\n",
    "    return my_format\n",
    "    \n",
    "def pie_chart(freqDist, keyword_dict, filepath):\n",
    "    topics = [\"environmental\", \"social\", \"governance\"]\n",
    "    counts = {\"environmental\": 0, \"social\": 0, \"governance\": 0}\n",
    "    for dist in freqDist:\n",
    "        word = dist[0]\n",
    "        count = dist[1]\n",
    "        for topic in topics:\n",
    "            if word.lower() in keyword_dict[topic]:\n",
    "                counts[topic] = counts[topic] + count\n",
    "    y = [counts[\"environmental\"], counts[\"social\"], counts[\"governance\"]]\n",
    "    y = np.array(y)\n",
    "    \n",
    "    textprops = {\"fontsize\":15}\n",
    "    colors=[\"forestgreen\",\"cornflowerblue\",\"darkorange\"]\n",
    "    # # autopct='%1.0f%%'\n",
    "    # print(counts)\n",
    "\n",
    "    plt.pie(y, labels=topics, autopct=autopct_format(y), colors=colors, textprops = textprops)\n",
    "    # plt.pie(y, labels=topics, colors=colors, textprops = textprops)\n",
    "    plt.savefig(filepath)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def saveWordCloud(text, company_name,types):\n",
    "    wordcloud = WordCloud(max_font_size=40).generate(text)\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    filepath = f\"WordClouds/{types}/{company_name}_{types}.png\"\n",
    "    plt.savefig(filepath,bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Unigram with Company Name (Create WordCloud, Frequency Distribution, PieChart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyzeUnigram(tokens, company_name):\n",
    "    no_token = len(tokens)\n",
    "    print(f\"The number of tokens is {no_token}\")\n",
    "    no_type = len(set(tokens))\n",
    "    print(f\"The number of type is {no_type}\")\n",
    "    \n",
    "\n",
    "    # Take out Stopwords from Tokens\n",
    "    allcontenttokens = [w.lower() for w in tokens if w.lower() not in stoplist and not bool(re.search(r'\\d', w))]\n",
    "    no_contenttoken = len(allcontenttokens)\n",
    "    print(f\"The number of content tokens is {no_contenttoken}\")\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    all_lemmas = [lemmatizer.lemmatize(w) for w in allcontenttokens]\n",
    "    print(f\"Unigram analysis for {company_name}\")\n",
    "    no_lemmas = len(all_lemmas)\n",
    "    print(f\"The number of lemmas is {no_lemmas}\\n\")\n",
    "    text = \" \".join(all_lemmas)\n",
    "    \n",
    "    # Create WordCloud\n",
    "    saveWordCloud(text, company_name,\"With_Company_Name\")\n",
    "\n",
    "    # Create Frequency Distribution\n",
    "    # freqdist = nltk.FreqDist(all_lemmas)\n",
    "    freqdist = print_fdist(all_lemmas)\n",
    "    freqdist = list(freqdist.items())\n",
    "\n",
    "    # Create a Pie Chart\n",
    "    pie_chart(freqdist, unigram_dict, f\"PieChartsUnigram/{company_name}.png\")\n",
    "    return no_token, no_type, no_contenttoken, no_lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Unigram without Company names (Create WordCloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyzeUnigram_without_companyname(tokens, company_name,clist):\n",
    "    # Take out Stopwords from Tokens\n",
    "    allcontenttokens = [w.lower() for w in tokens if w.lower() not in clist and not bool(re.search(r'\\d', w))]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    all_lemmas = [lemmatizer.lemmatize(w) for w in allcontenttokens]\n",
    "    text = \" \".join(all_lemmas)\n",
    "    # Create WordCloud\n",
    "    saveWordCloud(text,company_name,\"No_Company_Name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Bigrams (Create Frequency Distribution, PieChart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyzeBigram(tokens, company_name):\n",
    "    bigrams = nltk.ngrams(tokens, 2)\n",
    "    bigramlist = list(bigrams)\n",
    "    bigramlist_new = []\n",
    "\n",
    "    for bi in bigramlist:\n",
    "        if bi[0] in stoplist or bi[1] in stoplist:\n",
    "            continue\n",
    "        bigramlist_new.append(bi)\n",
    "        \n",
    "    print(f\"Bigram analysis for {company_name}\")\n",
    "    print(f\"The number of content bigrams is {len(bigramlist_new)}\")\n",
    "\n",
    "    bigramlist_stringify = [pair[0] + \" \" + pair[1] for pair in bigramlist_new]\n",
    "    # freqdist = nltk.FreqDist(bigramlist_stringify)\n",
    "    freqdist = print_fdist(bigramlist_stringify)\n",
    "    freqdist = list(freqdist.items())\n",
    "    \n",
    "    pie_chart(freqdist, bigram_dict, f\"PieChartsBigram/{company_name}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a file and replace it if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "def create_replace_file(foldername):\n",
    "    if os.path.exists(foldername):\n",
    "        shutil.rmtree(foldername)\n",
    "    os.makedirs(foldername)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create 1 txt file for each industry and each company  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Cisco text extracted\n",
      "\n",
      "\n",
      "Apple text extracted\n",
      "\n",
      "\n",
      "Amazon text extracted\n",
      "\n",
      "\n",
      "Technology text extraction complete\n",
      "\n",
      "\n",
      "Ross text extracted\n",
      "\n",
      "\n",
      "Target text extracted\n",
      "\n",
      "\n",
      "Tjx text extracted\n",
      "\n",
      "\n",
      "Retail text extraction complete\n",
      "\n",
      "\n",
      "GeneralMills text extracted\n",
      "\n",
      "\n",
      "Hershey text extracted\n",
      "\n",
      "\n",
      "KraftHeinz text extracted\n",
      "\n",
      "\n",
      "Food text extraction complete\n",
      "\n",
      "\n",
      "American Airline text extracted\n",
      "\n",
      "\n",
      "Delta text extracted\n",
      "\n",
      "\n",
      "Southwest Airlines text extracted\n",
      "\n",
      "\n",
      "Airlines text extraction complete\n",
      "\n",
      "\n",
      "Disney text extracted\n",
      "\n",
      "\n",
      "Netflix text extracted\n",
      "\n",
      "\n",
      "Fox text extracted\n",
      "\n",
      "\n",
      "Media text extraction complete\n"
     ]
    }
   ],
   "source": [
    "# main function\n",
    "company_list = []\n",
    "\n",
    "create_replace_file(\"Textfiles\")\n",
    "for industry in glob.iglob(f\"Reports/*\"):\n",
    "    industry_name = industry.split(\"/\")[-1]\n",
    "    \n",
    "    \n",
    "    ind_file = open(f\"Textfiles/{industry_name}.txt\", \"a\") # text file combining text from all industry companies\n",
    "    ## Create files for \n",
    "    os.makedirs(os.path.join(\"Textfiles\", industry_name), exist_ok=True)\n",
    "\n",
    "    for company in glob.iglob(f\"{industry}/*\"):\n",
    "        company_name = company.split(\"/\")[-1].strip(\".pdf\")\n",
    "        lowercase_company_name = company_name.lower()\n",
    "        company_list.append(lowercase_company_name)\n",
    "\n",
    "        # Extract the Text for each company\n",
    "        text = extractText(company)\n",
    "        with open(f\"Textfiles/{industry_name}/{company_name}.txt\", \"w\") as file:\n",
    "            file.write(text)\n",
    "            print(f\"\\n\\n{company_name} text extracted\")\n",
    "        ind_file.write(text)\n",
    "    print(f\"\\n\\n{industry_name} text extraction complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "pdf_texts = {}\n",
    "company_list = []\n",
    "\n",
    "for industry in glob.iglob(f\"Reports/*\"):\n",
    "    industry_name = industry.split(\"/\")[-1]\n",
    "    \n",
    "    \n",
    "    ind_file = open(f\"Textfiles/{industry_name}.txt\", \"a\") # text file combining text from all industry companies\n",
    "    ## Create files for \n",
    "\n",
    "    for company in glob.iglob(f\"{industry}/*\"):\n",
    "        company_name = company.split(\"/\")[-1].strip(\".pdf\")\n",
    "        lowercase_company_name = company_name.lower()\n",
    "        company_list.append(lowercase_company_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of company names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cisco', 'apple', 'amazon', 'ross', 'target', 'tjx', 'generalmills', 'hershey', 'kraftheinz', 'american airline', 'delta', 'southwest airlines', 'disney', 'netflix', 'fox']\n"
     ]
    }
   ],
   "source": [
    "print(company_list)\n",
    "company_list.extend([\"mills\",\"kraft\",\"heinz\",\"southwest\",\"american\",\"2022\",\"gid\",\"nr\"])\n",
    "\n",
    "stop_and_company_list =  stoplist+ company_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create diagrams for each company and industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textfiles/Technology\n",
      "Textfiles/Technology/Cisco.txt\n",
      "Starting analysis for Cisco\n",
      "The number of tokens is 24178\n",
      "The number of type is 3482\n",
      "The number of content tokens is 12382\n",
      "Unigram analysis for Cisco\n",
      "The number of lemmas is 12382\n",
      "\n",
      "('cisco', 370)\n",
      "('power', 117)\n",
      "('inclusive', 108)\n",
      "('esg', 106)\n",
      "('employee', 94)\n",
      "('purpose', 91)\n",
      "('hub', 89)\n",
      "('product', 87)\n",
      "('impact', 83)\n",
      "('report', 82)\n",
      "('goal', 76)\n",
      "('people', 74)\n",
      "('community', 71)\n",
      "('u', 70)\n",
      "('work', 66)\n",
      "('supplier', 65)\n",
      "('program', 61)\n",
      "('global', 60)\n",
      "('technology', 59)\n",
      "('future', 57)\n",
      "\n",
      "Bigram analysis for Cisco\n",
      "The number of content bigrams is 5286\n",
      "('purpose report', 59)\n",
      "('cisco purpose', 56)\n",
      "('intro power', 56)\n",
      "('power inclusive', 56)\n",
      "('inclusive cisco', 55)\n",
      "('human rights', 26)\n",
      "('social justice', 24)\n",
      "('future intro', 23)\n",
      "('networking academy', 20)\n",
      "('supply chain', 18)\n",
      "('circular design', 17)\n",
      "('inclusive future', 16)\n",
      "('ghg emissions', 16)\n",
      "('hybrid work', 16)\n",
      "('climate change', 15)\n",
      "('cisco networking', 15)\n",
      "('community impact', 15)\n",
      "('fy base', 14)\n",
      "('visit cisco', 13)\n",
      "('renewable energy', 12)\n",
      "\n",
      "Textfiles/Technology/Amazon.txt\n",
      "Starting analysis for Amazon\n",
      "The number of tokens is 48331\n",
      "The number of type is 5149\n",
      "The number of content tokens is 25897\n",
      "Unigram analysis for Amazon\n",
      "The number of lemmas is 25897\n",
      "\n",
      "('amazon', 535)\n",
      "('sustainability', 244)\n",
      "('product', 228)\n",
      "('employee', 209)\n",
      "('energy', 183)\n",
      "('customer', 156)\n",
      "('supplier', 139)\n",
      "('climate', 129)\n",
      "('carbon', 129)\n",
      "('program', 129)\n",
      "('emission', 125)\n",
      "('report', 119)\n",
      "('water', 116)\n",
      "('right', 115)\n",
      "('food', 115)\n",
      "('waste', 113)\n",
      "('renewable', 110)\n",
      "('people', 109)\n",
      "('support', 105)\n",
      "('device', 105)\n",
      "\n",
      "Bigram analysis for Amazon\n",
      "The number of content bigrams is 12495\n",
      "('sustainability report', 88)\n",
      "('renewable energy', 88)\n",
      "('amazon sustainability', 86)\n",
      "('human rights', 84)\n",
      "('climate pledge', 74)\n",
      "('introduction sustainability', 72)\n",
      "('private brands', 71)\n",
      "('supply chain', 62)\n",
      "('whole foods', 60)\n",
      "('foods market', 52)\n",
      "('report introduction', 42)\n",
      "('pledge friendly', 40)\n",
      "('carbon emissions', 38)\n",
      "('sustainability people', 37)\n",
      "('north america', 31)\n",
      "('amazon private', 30)\n",
      "('grocery private', 26)\n",
      "('mental health', 24)\n",
      "('climate solutions', 23)\n",
      "('net-zero carbon', 22)\n",
      "\n",
      "Textfiles/Technology/Apple.txt\n",
      "Starting analysis for Apple\n",
      "The number of tokens is 50003\n",
      "The number of type is 4626\n",
      "The number of content tokens is 25575\n",
      "Unigram analysis for Apple\n",
      "The number of lemmas is 25575\n",
      "\n",
      "('apple', 723)\n",
      "('supplier', 342)\n",
      "('employee', 236)\n",
      "('community', 233)\n",
      "('product', 216)\n",
      "('people', 203)\n",
      "('program', 203)\n",
      "('report', 188)\n",
      "('customer', 183)\n",
      "('support', 176)\n",
      "('team', 172)\n",
      "('environment', 161)\n",
      "('supply', 139)\n",
      "('work', 139)\n",
      "('chain', 135)\n",
      "('right', 123)\n",
      "('help', 119)\n",
      "('initiative', 113)\n",
      "('esg', 110)\n",
      "('governance', 108)\n",
      "\n",
      "Bigram analysis for Apple\n",
      "The number of content bigrams is 11063\n",
      "('supply chain', 127)\n",
      "('esg report', 85)\n",
      "('governance communities', 84)\n",
      "('communities suppliers', 84)\n",
      "('suppliers customers', 84)\n",
      "('environment introduction', 83)\n",
      "('appendix governance', 78)\n",
      "('introduction apple', 74)\n",
      "('team members', 71)\n",
      "('human rights', 70)\n",
      "('people environment', 69)\n",
      "('metric tons', 36)\n",
      "('supplier employees', 33)\n",
      "('apple watch', 27)\n",
      "('renewable electricity', 24)\n",
      "('pay equity', 23)\n",
      "('leadership roles', 22)\n",
      "('climate change', 20)\n",
      "('renewable energy', 19)\n",
      "('racial equity', 18)\n",
      "\n",
      "The number of tokens is 122512\n",
      "The number of type is 8316\n",
      "The number of content tokens is 63854\n",
      "Unigram analysis for Technology\n",
      "The number of lemmas is 63854\n",
      "\n",
      "('apple', 723)\n",
      "('supplier', 546)\n",
      "('employee', 539)\n",
      "('amazon', 536)\n",
      "('product', 531)\n",
      "('community', 395)\n",
      "('program', 393)\n",
      "('customer', 390)\n",
      "('report', 389)\n",
      "('people', 386)\n",
      "('cisco', 370)\n",
      "('support', 318)\n",
      "('energy', 305)\n",
      "('work', 292)\n",
      "('right', 281)\n",
      "('sustainability', 277)\n",
      "('supply', 272)\n",
      "('help', 266)\n",
      "('emission', 262)\n",
      "('chain', 253)\n",
      "\n",
      "Bigram analysis for Technology\n",
      "The number of content bigrams is 28844\n",
      "('supply chain', 207)\n",
      "('human rights', 180)\n",
      "('renewable energy', 119)\n",
      "('sustainability report', 88)\n",
      "('amazon sustainability', 86)\n",
      "('esg report', 85)\n",
      "('governance communities', 84)\n",
      "('communities suppliers', 84)\n",
      "('suppliers customers', 84)\n",
      "('environment introduction', 83)\n",
      "('appendix governance', 78)\n",
      "('climate pledge', 74)\n",
      "('introduction apple', 74)\n",
      "('introduction sustainability', 72)\n",
      "('team members', 72)\n",
      "('private brands', 71)\n",
      "('people environment', 70)\n",
      "('whole foods', 60)\n",
      "('purpose report', 59)\n",
      "('cisco purpose', 56)\n",
      "\n",
      "Textfiles/Retail\n",
      "Textfiles/Retail/Target.txt\n",
      "Starting analysis for Target\n",
      "The number of tokens is 60984\n",
      "The number of type is 4837\n",
      "The number of content tokens is 32728\n",
      "Unigram analysis for Target\n",
      "The number of lemmas is 32728\n",
      "\n",
      "('target', 730)\n",
      "('team', 411)\n",
      "('member', 324)\n",
      "('community', 289)\n",
      "('product', 277)\n",
      "('human', 246)\n",
      "('environmental', 241)\n",
      "('right', 241)\n",
      "('chain', 222)\n",
      "('esg', 220)\n",
      "('report', 219)\n",
      "('social', 216)\n",
      "('supply', 211)\n",
      "('emission', 193)\n",
      "('governance', 189)\n",
      "('gri', 177)\n",
      "('brand', 161)\n",
      "('guest', 157)\n",
      "('glossary', 156)\n",
      "('program', 155)\n",
      "\n",
      "Bigram analysis for Target\n",
      "The number of content bigrams is 15288\n",
      "('team members', 258)\n",
      "('human rights', 221)\n",
      "('supply chain', 175)\n",
      "('esg report', 112)\n",
      "('environmental social', 107)\n",
      "('governance indexes', 107)\n",
      "('target esg', 105)\n",
      "('introduction environmental', 103)\n",
      "('social governance', 101)\n",
      "('target forward', 89)\n",
      "('proxy statement', 86)\n",
      "('owned brand', 75)\n",
      "('fy fy', 59)\n",
      "('report introduction', 51)\n",
      "('core functions', 50)\n",
      "('net zero', 44)\n",
      "('report gri', 43)\n",
      "('esg issues', 43)\n",
      "('protecting human', 42)\n",
      "('ghg emissions', 42)\n",
      "\n",
      "Textfiles/Retail/Ross.txt\n",
      "Starting analysis for Ross\n",
      "The number of tokens is 19748\n",
      "The number of type is 2797\n",
      "The number of content tokens is 10216\n",
      "Unigram analysis for Ross\n",
      "The number of lemmas is 10216\n",
      "\n",
      "('ross', 204)\n",
      "('associate', 194)\n",
      "('store', 175)\n",
      "('community', 127)\n",
      "('corporate', 109)\n",
      "('program', 108)\n",
      "('report', 85)\n",
      "('social', 84)\n",
      "('responsibility', 82)\n",
      "('supporting', 82)\n",
      "('support', 82)\n",
      "('energy', 70)\n",
      "('operating', 66)\n",
      "('conducting', 64)\n",
      "('empowering', 63)\n",
      "('inc.', 61)\n",
      "('emission', 56)\n",
      "('sustainably', 50)\n",
      "('organization', 50)\n",
      "('product', 49)\n",
      "\n",
      "Bigram analysis for Ross\n",
      "The number of content bigrams is 4342\n",
      "('corporate social', 77)\n",
      "('social responsibility', 77)\n",
      "('ross stores', 68)\n",
      "('responsibility report', 59)\n",
      "('operating sustainably', 49)\n",
      "('inc. empowering', 44)\n",
      "('sustainably conducting', 44)\n",
      "('associates supporting', 35)\n",
      "('communities operating', 34)\n",
      "('distribution centers', 32)\n",
      "('ghg emissions', 23)\n",
      "('supply chain', 19)\n",
      "('first book', 18)\n",
      "('communities conducting', 14)\n",
      "('ethically operating', 14)\n",
      "('corporate governance', 14)\n",
      "('environmental sustainability', 13)\n",
      "('inc. supporting', 13)\n",
      "('ethically empowering', 13)\n",
      "('buying offices', 12)\n",
      "\n",
      "Textfiles/Retail/Tjx.txt\n",
      "Starting analysis for Tjx\n",
      "The number of tokens is 39345\n",
      "The number of type is 4188\n",
      "The number of content tokens is 20208\n",
      "Unigram analysis for Tjx\n",
      "The number of lemmas is 20208\n",
      "\n",
      "('global', 287)\n",
      "('tjx', 232)\n",
      "('associate', 224)\n",
      "('program', 215)\n",
      "('report', 213)\n",
      "('corporate', 177)\n",
      "('responsibility', 152)\n",
      "('support', 141)\n",
      "('vendor', 130)\n",
      "('help', 99)\n",
      "('opportunity', 97)\n",
      "('people', 95)\n",
      "('training', 92)\n",
      "('store', 91)\n",
      "('management', 90)\n",
      "('effort', 87)\n",
      "('social', 85)\n",
      "('compliance', 84)\n",
      "('code', 82)\n",
      "('product', 81)\n",
      "\n",
      "Bigram analysis for Tjx\n",
      "The number of content bigrams is 8759\n",
      "('corporate responsibility', 141)\n",
      "('global corporate', 122)\n",
      "('responsibility report', 113)\n",
      "('social compliance', 56)\n",
      "('cr report', 49)\n",
      "('young people', 43)\n",
      "('vendor code', 41)\n",
      "('global social', 35)\n",
      "('co e', 31)\n",
      "('environmental sustainability', 27)\n",
      "('compliance program', 26)\n",
      "('mt co', 25)\n",
      "('renewable energy', 22)\n",
      "('ghg emissions', 21)\n",
      "('distribution centers', 21)\n",
      "('vice president', 19)\n",
      "('vendors must', 19)\n",
      "('responsible sourcing', 18)\n",
      "('managerial positions', 18)\n",
      "('tjx canada', 17)\n",
      "\n",
      "The number of tokens is 120077\n",
      "The number of type is 7391\n",
      "The number of content tokens is 63152\n",
      "Unigram analysis for Retail\n",
      "The number of lemmas is 63152\n",
      "\n",
      "('target', 759)\n",
      "('report', 517)\n",
      "('community', 495)\n",
      "('team', 478)\n",
      "('program', 478)\n",
      "('associate', 424)\n",
      "('product', 407)\n",
      "('social', 385)\n",
      "('store', 372)\n",
      "('member', 369)\n",
      "('global', 343)\n",
      "('environmental', 339)\n",
      "('corporate', 334)\n",
      "('support', 323)\n",
      "('emission', 317)\n",
      "('responsibility', 285)\n",
      "('right', 284)\n",
      "('human', 282)\n",
      "('chain', 265)\n",
      "('energy', 261)\n",
      "\n",
      "Bigram analysis for Retail\n",
      "The number of content bigrams is 28390\n",
      "('team members', 258)\n",
      "('human rights', 242)\n",
      "('supply chain', 201)\n",
      "('responsibility report', 173)\n",
      "('corporate responsibility', 158)\n",
      "('global corporate', 122)\n",
      "('esg report', 112)\n",
      "('environmental social', 107)\n",
      "('governance indexes', 107)\n",
      "('target esg', 105)\n",
      "('introduction environmental', 103)\n",
      "('social governance', 101)\n",
      "('target forward', 89)\n",
      "('social responsibility', 89)\n",
      "('proxy statement', 88)\n",
      "('ghg emissions', 86)\n",
      "('corporate social', 77)\n",
      "('owned brand', 75)\n",
      "('fy fy', 74)\n",
      "('ross stores', 68)\n",
      "\n",
      "Textfiles/Food\n",
      "Textfiles/Food/GeneralMills.txt\n",
      "Starting analysis for GeneralMills\n",
      "The number of tokens is 50667\n",
      "The number of type is 5308\n",
      "The number of content tokens is 27907\n",
      "Unigram analysis for GeneralMills\n",
      "The number of lemmas is 27907\n",
      "\n",
      "('food', 455)\n",
      "('mill', 438)\n",
      "('general', 436)\n",
      "('n', 353)\n",
      "('global', 255)\n",
      "('impact', 197)\n",
      "('employee', 183)\n",
      "('people', 180)\n",
      "('program', 163)\n",
      "('product', 153)\n",
      "('supplier', 151)\n",
      "('water', 143)\n",
      "('support', 139)\n",
      "('work', 138)\n",
      "('regenerative', 130)\n",
      "('responsibility', 129)\n",
      "('planet', 126)\n",
      "('community', 122)\n",
      "('agriculture', 121)\n",
      "('climate', 106)\n",
      "\n",
      "Bigram analysis for GeneralMills\n",
      "The number of content bigrams is 13449\n",
      "('general mills', 422)\n",
      "('global responsibility', 100)\n",
      "('mills global', 87)\n",
      "('human rights', 86)\n",
      "('regenerative agriculture', 82)\n",
      "('food planet', 66)\n",
      "('planet people', 65)\n",
      "('responsibility food', 46)\n",
      "('climate change', 45)\n",
      "('supply chain', 43)\n",
      "('value chain', 39)\n",
      "('food safety', 34)\n",
      "('greenhouse gas', 32)\n",
      "('responsible sourcing', 31)\n",
      "('ghg emissions', 29)\n",
      "('global impact', 28)\n",
      "('food waste', 28)\n",
      "('water stewardship', 24)\n",
      "('palm oil', 24)\n",
      "('supplier code', 23)\n",
      "\n",
      "Textfiles/Food/KraftHeinz.txt\n",
      "Starting analysis for KraftHeinz\n",
      "The number of tokens is 36628\n",
      "The number of type is 4298\n",
      "The number of content tokens is 19868\n",
      "Unigram analysis for KraftHeinz\n",
      "The number of lemmas is 19868\n",
      "\n",
      "('heinz', 461)\n",
      "('kraft', 411)\n",
      "('esg', 186)\n",
      "('appendix', 162)\n",
      "('supplier', 157)\n",
      "('support', 144)\n",
      "('report', 142)\n",
      "('food', 141)\n",
      "('community', 139)\n",
      "('product', 127)\n",
      "('metric', 126)\n",
      "('animal', 123)\n",
      "('global', 119)\n",
      "('responsible', 114)\n",
      "('employee', 114)\n",
      "('sourcing', 112)\n",
      "('environmental', 111)\n",
      "('healthy', 103)\n",
      "('welfare', 102)\n",
      "('together', 96)\n",
      "\n",
      "Bigram analysis for KraftHeinz\n",
      "The number of content bigrams is 9629\n",
      "('kraft heinz', 388)\n",
      "('responsible sourcing', 94)\n",
      "('animal welfare', 90)\n",
      "('environmental stewardship', 89)\n",
      "('healthy living', 89)\n",
      "('esg report', 88)\n",
      "('community support', 88)\n",
      "('heinz esg', 85)\n",
      "('appendix environmental', 80)\n",
      "('stewardship responsible', 80)\n",
      "('sourcing healthy', 80)\n",
      "('support appendix', 80)\n",
      "('appendix introduction', 74)\n",
      "('report appendix', 54)\n",
      "('risk assessment', 45)\n",
      "('welfare risk', 39)\n",
      "('supplier self-reporting', 37)\n",
      "('heinz animal', 37)\n",
      "('metric tons', 26)\n",
      "('fb-mp- a.', 25)\n",
      "\n",
      "Textfiles/Food/Hershey.txt\n",
      "Starting analysis for Hershey\n",
      "The number of tokens is 68021\n",
      "The number of type is 5802\n",
      "The number of content tokens is 36181\n",
      "Unigram analysis for Hershey\n",
      "The number of lemmas is 36181\n",
      "\n",
      "('hershey', 687)\n",
      "('right', 332)\n",
      "('report', 304)\n",
      "('human', 301)\n",
      "('employee', 279)\n",
      "('—our', 266)\n",
      "('child', 232)\n",
      "('sourcing', 226)\n",
      "('esg', 223)\n",
      "('people', 223)\n",
      "('cocoa', 222)\n",
      "('program', 218)\n",
      "('water', 213)\n",
      "('community', 204)\n",
      "('good', 201)\n",
      "('supplier', 197)\n",
      "('emission', 175)\n",
      "('chain', 159)\n",
      "('scope', 155)\n",
      "('labor', 147)\n",
      "\n",
      "Bigram analysis for Hershey\n",
      "The number of content bigrams is 16642\n",
      "('human rights', 285)\n",
      "('contents —making', 133)\n",
      "('—making good', 133)\n",
      "('good —our', 133)\n",
      "('—cocoa —responsible', 133)\n",
      "('—responsible sourcing', 133)\n",
      "('rights —environment', 133)\n",
      "('—environment —our', 133)\n",
      "('—our people', 133)\n",
      "('people —youth', 133)\n",
      "('—youth —community', 133)\n",
      "('—community —about', 133)\n",
      "('esg report', 113)\n",
      "('supply chain', 112)\n",
      "('report contents', 100)\n",
      "('child labor', 75)\n",
      "('ghg emissions', 54)\n",
      "('responsible sourcing', 45)\n",
      "('food safety', 42)\n",
      "('climate change', 38)\n",
      "\n",
      "The number of tokens is 155316\n",
      "The number of type is 9571\n",
      "The number of content tokens is 83956\n",
      "Unigram analysis for Food\n",
      "The number of lemmas is 83956\n",
      "\n",
      "('food', 715)\n",
      "('hershey', 687)\n",
      "('employee', 576)\n",
      "('report', 519)\n",
      "('supplier', 505)\n",
      "('right', 470)\n",
      "('community', 465)\n",
      "('global', 464)\n",
      "('heinz', 461)\n",
      "('mill', 458)\n",
      "('program', 455)\n",
      "('water', 452)\n",
      "('general', 442)\n",
      "('people', 439)\n",
      "('human', 429)\n",
      "('sourcing', 426)\n",
      "('esg', 415)\n",
      "('kraft', 411)\n",
      "('support', 399)\n",
      "('product', 398)\n",
      "\n",
      "Bigram analysis for Food\n",
      "The number of content bigrams is 39722\n",
      "('general mills', 422)\n",
      "('human rights', 392)\n",
      "('kraft heinz', 388)\n",
      "('esg report', 201)\n",
      "('supply chain', 179)\n",
      "('responsible sourcing', 170)\n",
      "('contents —making', 133)\n",
      "('—making good', 133)\n",
      "('good —our', 133)\n",
      "('—cocoa —responsible', 133)\n",
      "('—responsible sourcing', 133)\n",
      "('rights —environment', 133)\n",
      "('—environment —our', 133)\n",
      "('—our people', 133)\n",
      "('people —youth', 133)\n",
      "('—youth —community', 133)\n",
      "('—community —about', 133)\n",
      "('animal welfare', 106)\n",
      "('global responsibility', 100)\n",
      "('report contents', 100)\n",
      "\n",
      "Textfiles/Airlines\n",
      "Textfiles/Airlines/American Airline.txt\n",
      "Starting analysis for American Airline\n",
      "The number of tokens is 37588\n",
      "The number of type is 4363\n",
      "The number of content tokens is 18924\n",
      "Unigram analysis for American Airline\n",
      "The number of lemmas is 18924\n",
      "\n",
      "('american', 342)\n",
      "('emission', 234)\n",
      "('team', 226)\n",
      "('member', 210)\n",
      "('sustainability', 203)\n",
      "('safety', 184)\n",
      "('fuel', 174)\n",
      "('airline', 166)\n",
      "('customer', 154)\n",
      "('climate', 147)\n",
      "('risk', 145)\n",
      "('change', 138)\n",
      "('strategy', 122)\n",
      "('report', 120)\n",
      "('aircraft', 111)\n",
      "('index', 96)\n",
      "('scope', 91)\n",
      "('flight', 89)\n",
      "('operation', 84)\n",
      "('saf', 83)\n",
      "\n",
      "Bigram analysis for American Airline\n",
      "The number of content bigrams is 8899\n",
      "('team members', 170)\n",
      "('american airlines', 115)\n",
      "('climate change', 114)\n",
      "('sustainability strategy', 80)\n",
      "('sustainability report', 75)\n",
      "('ceo message', 74)\n",
      "('airlines sustainability', 70)\n",
      "('change safety', 67)\n",
      "('strategy ceo', 67)\n",
      "('safety sustainability', 61)\n",
      "('customers team', 57)\n",
      "('members climate', 57)\n",
      "('report indexes', 53)\n",
      "('jet fuel', 44)\n",
      "('scope emissions', 39)\n",
      "('team member', 27)\n",
      "('ghg emissions', 27)\n",
      "('net zero', 25)\n",
      "('metric tons', 25)\n",
      "('message indexes', 24)\n",
      "\n",
      "Textfiles/Airlines/Delta.txt\n",
      "Starting analysis for Delta\n",
      "The number of tokens is 36035\n",
      "The number of type is 4592\n",
      "The number of content tokens is 19739\n",
      "Unigram analysis for Delta\n",
      "The number of lemmas is 19739\n",
      "\n",
      "('delta', 301)\n",
      "('climate', 242)\n",
      "('emission', 191)\n",
      "('safety', 187)\n",
      "('fuel', 169)\n",
      "('employee', 156)\n",
      "('people', 141)\n",
      "('community', 123)\n",
      "('saf', 121)\n",
      "('lobbying', 111)\n",
      "('policy', 111)\n",
      "('program', 109)\n",
      "('goal', 109)\n",
      "('environment', 108)\n",
      "('governance', 99)\n",
      "('risk', 98)\n",
      "('appendix', 84)\n",
      "('customer', 84)\n",
      "('operation', 84)\n",
      "('introduction', 82)\n",
      "\n",
      "Bigram analysis for Delta\n",
      "The number of content bigrams is 9446\n",
      "('climate lobbying', 96)\n",
      "('introduction safety', 80)\n",
      "('safety people', 80)\n",
      "('people environment', 80)\n",
      "('environment climate', 80)\n",
      "('lobbying community', 80)\n",
      "('community governance', 80)\n",
      "('governance appendix', 80)\n",
      "('climate change', 39)\n",
      "('jet fuel', 38)\n",
      "('ghg emissions', 37)\n",
      "('climate goals', 29)\n",
      "('supply chain', 27)\n",
      "('fuel efficiency', 22)\n",
      "('climate policy', 17)\n",
      "('delta air', 16)\n",
      "('emissions intensity', 16)\n",
      "('lobbying activities', 16)\n",
      "('trade associations', 16)\n",
      "('air lines', 15)\n",
      "\n",
      "Textfiles/Airlines/Southwest Airlines.txt\n",
      "Starting analysis for Southwest Airlines\n",
      "The number of tokens is 63642\n",
      "The number of type is 5492\n",
      "The number of content tokens is 33504\n",
      "Unigram analysis for Southwest Airlines\n",
      "The number of lemmas is 33504\n",
      "\n",
      "('southwest', 772)\n",
      "('employee', 465)\n",
      "('airline', 435)\n",
      "('report', 326)\n",
      "('governance', 305)\n",
      "('performance', 287)\n",
      "('reported', 275)\n",
      "('planet', 271)\n",
      "('people', 266)\n",
      "('framework', 248)\n",
      "('fuel', 231)\n",
      "('citizenship', 204)\n",
      "('impact', 200)\n",
      "('risk', 192)\n",
      "('customer', 188)\n",
      "('goal', 181)\n",
      "('program', 170)\n",
      "('emission', 167)\n",
      "('saf', 155)\n",
      "('operation', 145)\n",
      "\n",
      "Bigram analysis for Southwest Airlines\n",
      "The number of content bigrams is 15546\n",
      "('southwest airlines', 367)\n",
      "('governance frameworks', 170)\n",
      "('performance planet', 168)\n",
      "('planet governance', 168)\n",
      "('frameworks citizenship', 155)\n",
      "('pb southwest', 136)\n",
      "('people performance', 126)\n",
      "('report pb', 76)\n",
      "('jet fuel', 64)\n",
      "('report people', 54)\n",
      "('g g', 46)\n",
      "('management approach', 44)\n",
      "('people people', 44)\n",
      "('reportpeople performance', 43)\n",
      "('net zero', 40)\n",
      "('governance southwest', 39)\n",
      "('report governance', 39)\n",
      "('governance people', 39)\n",
      "('supply chain', 38)\n",
      "('carbon emissions', 31)\n",
      "\n",
      "The number of tokens is 137265\n",
      "The number of type is 8685\n",
      "The number of content tokens is 72167\n",
      "Unigram analysis for Airlines\n",
      "The number of lemmas is 72167\n",
      "\n",
      "('southwest', 772)\n",
      "('employee', 677)\n",
      "('airline', 656)\n",
      "('emission', 592)\n",
      "('fuel', 574)\n",
      "('report', 504)\n",
      "('climate', 483)\n",
      "('safety', 452)\n",
      "('risk', 435)\n",
      "('people', 427)\n",
      "('customer', 426)\n",
      "('governance', 414)\n",
      "('american', 368)\n",
      "('saf', 359)\n",
      "('performance', 358)\n",
      "('team', 353)\n",
      "('program', 343)\n",
      "('goal', 339)\n",
      "('sustainability', 330)\n",
      "('operation', 313)\n",
      "\n",
      "Bigram analysis for Airlines\n",
      "The number of content bigrams is 33891\n",
      "('southwest airlines', 367)\n",
      "('climate change', 176)\n",
      "('team members', 172)\n",
      "('governance frameworks', 170)\n",
      "('performance planet', 168)\n",
      "('planet governance', 168)\n",
      "('frameworks citizenship', 155)\n",
      "('jet fuel', 146)\n",
      "('pb southwest', 136)\n",
      "('people performance', 126)\n",
      "('american airlines', 115)\n",
      "('climate lobbying', 96)\n",
      "('sustainability strategy', 86)\n",
      "('ghg emissions', 86)\n",
      "('introduction safety', 80)\n",
      "('safety people', 80)\n",
      "('people environment', 80)\n",
      "('environment climate', 80)\n",
      "('lobbying community', 80)\n",
      "('community governance', 80)\n",
      "\n",
      "Textfiles/Media\n",
      "Textfiles/Media/Disney.txt\n",
      "Starting analysis for Disney\n",
      "The number of tokens is 42160\n",
      "The number of type is 4426\n",
      "The number of content tokens is 21642\n",
      "Unigram analysis for Disney\n",
      "The number of lemmas is 21642\n",
      "\n",
      "('disney', 432)\n",
      "('world', 411)\n",
      "('employee', 186)\n",
      "('program', 177)\n",
      "('people', 165)\n",
      "('framework', 142)\n",
      "('approach', 129)\n",
      "('community', 124)\n",
      "('hope', 116)\n",
      "('operating', 114)\n",
      "('belonging', 109)\n",
      "('investing', 109)\n",
      "('responsibly', 107)\n",
      "('balance', 106)\n",
      "('emission', 103)\n",
      "('introduction', 101)\n",
      "('content', 99)\n",
      "('support', 96)\n",
      "('child', 85)\n",
      "('safety', 77)\n",
      "\n",
      "Bigram analysis for Disney\n",
      "The number of content bigrams is 9637\n",
      "('operating responsibly', 105)\n",
      "('approach introduction', 97)\n",
      "('contents world', 97)\n",
      "('introduction world', 83)\n",
      "('people operating', 78)\n",
      "('hope investing', 75)\n",
      "('balance world', 67)\n",
      "('walt disney', 64)\n",
      "('supply chain', 31)\n",
      "('disney world', 31)\n",
      "('human rights', 30)\n",
      "('cast members', 30)\n",
      "('abc news', 25)\n",
      "('charitable giving', 24)\n",
      "('disney voluntears', 20)\n",
      "('disney aspire', 20)\n",
      "('scope emissions', 20)\n",
      "('world resort', 19)\n",
      "('disney conservation', 17)\n",
      "('national geographic', 17)\n",
      "\n",
      "Textfiles/Media/Fox.txt\n",
      "Starting analysis for Fox\n",
      "The number of tokens is 23992\n",
      "The number of type is 3942\n",
      "The number of content tokens is 13541\n",
      "Unigram analysis for Fox\n",
      "The number of lemmas is 13541\n",
      "\n",
      "('fox', 527)\n",
      "('forward', 200)\n",
      "('employee', 141)\n",
      "('program', 131)\n",
      "('community', 110)\n",
      "('policy', 105)\n",
      "('foundation', 97)\n",
      "('inc', 92)\n",
      "('sport', 84)\n",
      "('news', 74)\n",
      "('support', 65)\n",
      "('medium', 62)\n",
      "('people', 60)\n",
      "('approach', 52)\n",
      "('highlight', 47)\n",
      "('environmental', 47)\n",
      "('service', 46)\n",
      "('station', 45)\n",
      "('los', 45)\n",
      "('initiative', 44)\n",
      "\n",
      "Bigram analysis for Fox\n",
      "The number of content bigrams is 6539\n",
      "('community forward', 45)\n",
      "('highlights community', 43)\n",
      "('fox news', 43)\n",
      "('policy forward', 41)\n",
      "('los angeles', 41)\n",
      "('people forward', 40)\n",
      "('fox sports', 38)\n",
      "('sustainably forward', 35)\n",
      "('forward policy', 34)\n",
      "('forwardsasb appendix', 32)\n",
      "('forward people', 30)\n",
      "('red cross', 28)\n",
      "('fox corporation', 27)\n",
      "('news media', 26)\n",
      "('forward sustainably', 26)\n",
      "('television stations', 17)\n",
      "('fox studio', 17)\n",
      "('forward giving', 16)\n",
      "('fox forward', 16)\n",
      "('studio lot', 16)\n",
      "\n",
      "Textfiles/Media/Netflix.txt\n",
      "Starting analysis for Netflix\n",
      "The number of tokens is 20491\n",
      "The number of type is 2977\n",
      "The number of content tokens is 10964\n",
      "Unigram analysis for Netflix\n",
      "The number of lemmas is 10964\n",
      "\n",
      "('netflix', 210)\n",
      "('emission', 164)\n",
      "('report', 144)\n",
      "('governance', 127)\n",
      "('risk', 127)\n",
      "('sustainability', 121)\n",
      "('esg', 114)\n",
      "('social', 96)\n",
      "('climate', 87)\n",
      "('environment', 86)\n",
      "('appendix', 82)\n",
      "('introduction', 75)\n",
      "('scope', 74)\n",
      "('employee', 66)\n",
      "('energy', 64)\n",
      "('carbon', 63)\n",
      "('board', 60)\n",
      "('production', 54)\n",
      "('value', 53)\n",
      "('director', 53)\n",
      "\n",
      "Bigram analysis for Netflix\n",
      "The number of content bigrams is 5447\n",
      "('esg report', 80)\n",
      "('environment governance', 74)\n",
      "('governance social', 74)\n",
      "('social introduction', 74)\n",
      "('introduction appendix', 74)\n",
      "('netflix esg', 72)\n",
      "('appendix netflix', 60)\n",
      "('value chain', 40)\n",
      "('climate risk', 40)\n",
      "('sustainability strategy', 31)\n",
      "('remaining emissions', 30)\n",
      "('netting remaining', 28)\n",
      "('renewable energy', 28)\n",
      "('greenhouse gas', 28)\n",
      "('strategy sustainability', 27)\n",
      "('chain netting', 27)\n",
      "('zero sustainability', 27)\n",
      "('risk environment', 26)\n",
      "('storytelling climate', 21)\n",
      "('responsible products', 20)\n",
      "\n",
      "The number of tokens is 86643\n",
      "The number of type is 7282\n",
      "The number of content tokens is 46147\n",
      "Unigram analysis for Media\n",
      "The number of lemmas is 46147\n",
      "\n",
      "('fox', 529)\n",
      "('world', 447)\n",
      "('disney', 433)\n",
      "('employee', 393)\n",
      "('program', 342)\n",
      "('emission', 289)\n",
      "('community', 261)\n",
      "('people', 238)\n",
      "('report', 222)\n",
      "('risk', 214)\n",
      "('netflix', 211)\n",
      "('introduction', 210)\n",
      "('forward', 204)\n",
      "('approach', 200)\n",
      "('policy', 186)\n",
      "('support', 178)\n",
      "('framework', 156)\n",
      "('governance', 151)\n",
      "('sustainability', 151)\n",
      "('social', 145)\n",
      "\n",
      "Bigram analysis for Media\n",
      "The number of content bigrams is 21623\n",
      "('operating responsibly', 105)\n",
      "('approach introduction', 97)\n",
      "('contents world', 97)\n",
      "('introduction world', 83)\n",
      "('esg report', 80)\n",
      "('people operating', 78)\n",
      "('hope investing', 75)\n",
      "('environment governance', 74)\n",
      "('governance social', 74)\n",
      "('social introduction', 74)\n",
      "('introduction appendix', 74)\n",
      "('netflix esg', 72)\n",
      "('balance world', 67)\n",
      "('walt disney', 64)\n",
      "('appendix netflix', 60)\n",
      "('los angeles', 49)\n",
      "('community forward', 45)\n",
      "('climate risk', 43)\n",
      "('highlights community', 43)\n",
      "('fox news', 43)\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"WordClouds\", exist_ok=True)\n",
    "os.makedirs(\"PieChartsUnigram\", exist_ok=True)\n",
    "os.makedirs(\"PieChartsBigram\", exist_ok=True)\n",
    "\n",
    "comp_list = []\n",
    "industry_list = []\n",
    "for industry in glob.iglob(\"Textfiles/*\"):\n",
    "    \n",
    "    \n",
    "    if os.path.isdir(industry):\n",
    "        print(industry)\n",
    "        industry_name = industry.split(\"/\")[-1]\n",
    "        industry_tokens = []\n",
    "        os.makedirs(os.path.join(\"WordClouds\", \"With_Company_Name\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(\"WordClouds\", \"No_Company_Name\"), exist_ok=True)\n",
    "        \n",
    "        for company in glob.iglob(f\"{industry}/*\"):\n",
    "            print(company)\n",
    "\n",
    "            company_name = company.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "\n",
    "            # get text from txt file of words extracted from pdf reports\n",
    "            with open(f\"Textfiles/{industry_name}/{company_name}.txt\", \"r\") as file:\n",
    "                text = file.read()\n",
    "\n",
    "            # Save the content tokens for each company\n",
    "            print(f\"Starting analysis for {company_name}\")\n",
    "            text_tokens = alltokens(text)\n",
    "            industry_tokens.extend(text_tokens)\n",
    "            \n",
    "            \n",
    "            comp_token, comp_type, comp_contenttoken, comp_lemmas = analyzeUnigram(text_tokens, company_name)\n",
    "            comp_list.append({'Industry': industry_name, 'Company': company_name, 'Token': comp_token, 'Type': comp_type, 'Content Token':comp_contenttoken, 'Lemmas':comp_lemmas})\n",
    "            \n",
    "            analyzeUnigram_without_companyname(text_tokens, company_name,stop_and_company_list)\n",
    "            analyzeBigram(text_tokens, company_name)\n",
    "            \n",
    "        industry_token, industry_type, industry_contenttoken, industry_lemmas = analyzeUnigram(industry_tokens, industry_name)\n",
    "        industry_list.append({'Industry': industry_name, 'Token': industry_token, 'Type': industry_type, 'Content Token':industry_contenttoken, 'Lemmas':industry_lemmas})\n",
    "        \n",
    "        analyzeUnigram_without_companyname(industry_tokens, industry_name, stop_and_company_list)\n",
    "        analyzeBigram(industry_tokens, industry_name)\n",
    "\n",
    "#Create CSV file so easier to compare numbers and transfer to Goggle Slides\n",
    "comp_df = pd.DataFrame(comp_list)\n",
    "comp_df.to_csv(\"company_stats.csv\", index=False)\n",
    "\n",
    "industry_df = pd.DataFrame(industry_list)\n",
    "industry_df.to_csv(\"industry_stats.csv\", index=False)\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF for each industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "text_files = glob.glob(\"Textfiles/*.txt\")\n",
    "file_names = [Path(text).stem for text in text_files]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english')\n",
    "tfidf_vector = tfidf_vectorizer.fit_transform(text_files)\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=file_names, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_df = tfidf_df.drop(columns=company_list, errors='ignore')\n",
    "# tfidf_df.stack().reset_index()\n",
    "tfidf_df = tfidf_df.stack().reset_index()\n",
    "\n",
    "tfidf_df = tfidf_df.rename(columns={0:'tfidf', 'level_0': 'document','level_1': 'term', 'level_2': 'term'})\n",
    "tfidf_df = tfidf_df[~tfidf_df['term'].isin(company_list)]\n",
    "tfidf_df.sort_values(by=['document','tfidf'], ascending=[True,False]).groupby(['document']).head(10)\n",
    "top_tfidf = tfidf_df.sort_values(by=['document','tfidf'], ascending=[True,False]).groupby(['document']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-8c455db04a4b453680bd1a079f0b9428.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-8c455db04a4b453680bd1a079f0b9428.vega-embed details,\n",
       "  #altair-viz-8c455db04a4b453680bd1a079f0b9428.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-8c455db04a4b453680bd1a079f0b9428\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-8c455db04a4b453680bd1a079f0b9428\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-8c455db04a4b453680bd1a079f0b9428\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.15.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.15.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": {\"type\": \"rect\"}, \"encoding\": {\"color\": {\"field\": \"tfidf\", \"type\": \"quantitative\"}, \"x\": {\"field\": \"rank\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"document\", \"type\": \"nominal\"}}, \"transform\": [{\"window\": [{\"op\": \"rank\", \"field\": \"\", \"as\": \"rank\"}], \"groupby\": [\"document\"], \"sort\": [{\"field\": \"tfidf\", \"order\": \"descending\"}]}]}, {\"mark\": {\"type\": \"text\", \"baseline\": \"middle\", \"fontSize\": 14}, \"encoding\": {\"color\": {\"condition\": {\"test\": \"(datum.tfidf >= 0.23)\", \"value\": \"white\"}, \"value\": \"black\"}, \"text\": {\"field\": \"term\", \"type\": \"nominal\"}, \"x\": {\"field\": \"rank\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"document\", \"type\": \"nominal\"}}, \"transform\": [{\"window\": [{\"op\": \"rank\", \"field\": \"\", \"as\": \"rank\"}], \"groupby\": [\"document\"], \"sort\": [{\"field\": \"tfidf\", \"order\": \"descending\"}]}]}], \"data\": {\"name\": \"data-a523abf47b599f4805a1b8aa5073fbd9\"}, \"height\": 350, \"width\": 900, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.15.1.json\", \"datasets\": {\"data-a523abf47b599f4805a1b8aa5073fbd9\": [{\"document\": \"Airlines\", \"term\": \"airlines\", \"tfidf\": 0.25321582573586127}, {\"document\": \"Airlines\", \"term\": \"climate\", \"tfidf\": 0.16352765974597377}, {\"document\": \"Airlines\", \"term\": \"emissions\", \"tfidf\": 0.15670176536571692}, {\"document\": \"Airlines\", \"term\": \"fuel\", \"tfidf\": 0.15612575456795727}, {\"document\": \"Airlines\", \"term\": \"saf\", \"tfidf\": 0.1410380343471075}, {\"document\": \"Airlines\", \"term\": \"employees\", \"tfidf\": 0.13317698554402102}, {\"document\": \"Airlines\", \"term\": \"report\", \"tfidf\": 0.12772388629073456}, {\"document\": \"Airlines\", \"term\": \"safety\", \"tfidf\": 0.12553318515111828}, {\"document\": \"Airlines\", \"term\": \"people\", \"tfidf\": 0.11841757808180313}, {\"document\": \"Airlines\", \"term\": \"aircraft\", \"tfidf\": 0.11527175280792641}, {\"document\": \"Food\", \"term\": \"food\", \"tfidf\": 0.18694147234339578}, {\"document\": \"Food\", \"term\": \"cocoa\", \"tfidf\": 0.1833630713023059}, {\"document\": \"Food\", \"term\": \"company\", \"tfidf\": 0.1537836151702985}, {\"document\": \"Food\", \"term\": \"report\", \"tfidf\": 0.12801511099647458}, {\"document\": \"Food\", \"term\": \"global\", \"tfidf\": 0.12640893480504062}, {\"document\": \"Food\", \"term\": \"water\", \"tfidf\": 0.12554891197837284}, {\"document\": \"Food\", \"term\": \"general\", \"tfidf\": 0.12038967093788207}, {\"document\": \"Food\", \"term\": \"people\", \"tfidf\": 0.1204840559931022}, {\"document\": \"Food\", \"term\": \"rights\", \"tfidf\": 0.11773479760089736}, {\"document\": \"Food\", \"term\": \"esg\", \"tfidf\": 0.11581698939141012}, {\"document\": \"Media\", \"term\": \"world\", \"tfidf\": 0.19553752587055492}, {\"document\": \"Media\", \"term\": \"data\", \"tfidf\": 0.17685880870226137}, {\"document\": \"Media\", \"term\": \"employees\", \"tfidf\": 0.12560473579633083}, {\"document\": \"Media\", \"term\": \"emissions\", \"tfidf\": 0.11601271049894929}, {\"document\": \"Media\", \"term\": \"people\", \"tfidf\": 0.10431812493192937}, {\"document\": \"Media\", \"term\": \"program\", \"tfidf\": 0.10343611252452203}, {\"document\": \"Media\", \"term\": \"fiscal\", \"tfidf\": 0.10215849238199781}, {\"document\": \"Media\", \"term\": \"forward\", \"tfidf\": 0.09516968595258087}, {\"document\": \"Media\", \"term\": \"introduction\", \"tfidf\": 0.09434062279719929}, {\"document\": \"Media\", \"term\": \"report\", \"tfidf\": 0.08691119389632704}, {\"document\": \"Retail\", \"term\": \"report\", \"tfidf\": 0.19810938636272787}, {\"document\": \"Retail\", \"term\": \"business\", \"tfidf\": 0.1695854315571579}, {\"document\": \"Retail\", \"term\": \"associates\", \"tfidf\": 0.16861656805233705}, {\"document\": \"Retail\", \"term\": \"team\", \"tfidf\": 0.1592036201467902}, {\"document\": \"Retail\", \"term\": \"social\", \"tfidf\": 0.1488303509069143}, {\"document\": \"Retail\", \"term\": \"communities\", \"tfidf\": 0.13681376650157173}, {\"document\": \"Retail\", \"term\": \"global\", \"tfidf\": 0.13218882037396115}, {\"document\": \"Retail\", \"term\": \"environmental\", \"tfidf\": 0.13147001800437597}, {\"document\": \"Retail\", \"term\": \"corporate\", \"tfidf\": 0.12876620843494613}, {\"document\": \"Retail\", \"term\": \"glossary\", \"tfidf\": 0.12626488371176767}, {\"document\": \"Technology\", \"term\": \"employees\", \"tfidf\": 0.14059030383967555}, {\"document\": \"Technology\", \"term\": \"people\", \"tfidf\": 0.13816829239440417}, {\"document\": \"Technology\", \"term\": \"products\", \"tfidf\": 0.13202082462617995}, {\"document\": \"Technology\", \"term\": \"report\", \"tfidf\": 0.1274212818452164}, {\"document\": \"Technology\", \"term\": \"suppliers\", \"tfidf\": 0.11853427687773717}, {\"document\": \"Technology\", \"term\": \"energy\", \"tfidf\": 0.11669680962841435}, {\"document\": \"Technology\", \"term\": \"customers\", \"tfidf\": 0.11138047167037167}, {\"document\": \"Technology\", \"term\": \"support\", \"tfidf\": 0.10632850061703597}, {\"document\": \"Technology\", \"term\": \"work\", \"tfidf\": 0.10211050055703647}, {\"document\": \"Technology\", \"term\": \"year\", \"tfidf\": 0.1014067084731239}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a heat map\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# adding a little randomness to break ties in term ranking\n",
    "top_tfidf_plusRand = top_tfidf.copy()\n",
    "top_tfidf_plusRand['tfidf'] = top_tfidf_plusRand['tfidf'] + np.random.rand(top_tfidf.shape[0])*0.0001\n",
    "\n",
    "# base for all visualizations, with rank calculation\n",
    "base = alt.Chart(top_tfidf_plusRand).encode(\n",
    "    x = 'rank:O',\n",
    "    y = 'document:N'\n",
    ").transform_window(\n",
    "    rank = \"rank()\",\n",
    "    sort = [alt.SortField(\"tfidf\", order=\"descending\")],\n",
    "    groupby = [\"document\"],\n",
    ")\n",
    "\n",
    "# heatmap specification\n",
    "heatmap = base.mark_rect().encode(\n",
    "    color = 'tfidf:Q'\n",
    ")\n",
    "\n",
    "# text labels, white for darker heatmap colors\n",
    "text = base.mark_text(baseline='middle', fontSize=14).encode(\n",
    "    text = 'term:N',\n",
    "    color = alt.condition(alt.datum.tfidf >= 0.23, alt.value('white'), alt.value('black'))\n",
    ")\n",
    "\n",
    "# display the three superimposed visualizations\n",
    "(heatmap + text).properties(width=900,height = 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
